// supabase/functions/knowledge-task/index.ts

import { serve } from "https://deno.land/std@0.177.0/http/server.ts";
import { downloadFile } from "./service-r2.ts";
import { extractText } from "./extractor-files.ts";
import { splitChunks, cleanText } from "./utils-text.ts";
import { generateEmbeddings } from "./service-embeddings.ts";
import { SupabaseClient } from "./service-supabase.ts";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

// Segmented processing: ÊØèÊ¨°Â§ÑÁêÜ500‰∏™chunks
const CHUNK_LIMIT = 500;

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders });
  }

  let step = "init";
  let fileId = "";

  try {
    step = "parse request";
    const { record } = await req.json();
    fileId = record.id;
    const db = new SupabaseClient();

    console.log(`
${'='.repeat(60)}`);
    console.log(`üìÑ Processing Knowledge Upload: ${record.file_name} (ID: ${fileId})`);
    console.log(`üë§ User ID: ${record.user_id}`);
    console.log(`üìÇ Source: ${record.metadata?.source || 'file_upload'}`);
    console.log(`${'='.repeat(60)}
`);

    let text: string;

    // Check if it's a manual entry
    if (record.metadata?.source === 'manual_entry' && record.metadata?.manual_content) {
      step = "extract manual content";
      console.log(`‚úçÔ∏è  Step 1: Using manual content (skipping R2 download)`);
      text = record.metadata.manual_content;
      console.log(`‚úÖ Content length: ${text.length} characters`);
    } else {
      // Original file download logic
      step = "download file";
      console.log(`üì• Step 1: Downloading file from R2...`);
      const buffer = await downloadFile(record.storage_path);
      console.log(`‚úÖ Downloaded: ${(buffer.byteLength / 1024).toFixed(2)} KB`);

      step = "extract text";
      console.log(`
üìù Step 2: Extracting text...`);
      text = await extractText(buffer, record.file_name);
      console.log(`‚úÖ Extracted: ${text.length} characters`);
    }

    // Step 3: Clean and split text (‰ΩøÁî®Êñ∞ÁöÑ LangChain splitter)
    step = "process chunks";
    console.log(`
‚úÇÔ∏è  Step 3: Cleaning and splitting text...`);
    const cleanedText = cleanText(text);
    const allChunks = await splitChunks(cleanedText);
    console.log(`‚úÖ Created ${allChunks.length} chunks (total)`);

    // Segmented processing: Ëé∑ÂèñÂ∑≤Â§ÑÁêÜÁöÑchunksÊï∞Èáè
    const processedCount = record.metadata?.processed_chunks || 0;
    console.log(`üìä Already processed: ${processedCount} chunks`);

    // ËÆ°ÁÆóÊú¨Ê¨°Ë¶ÅÂ§ÑÁêÜÁöÑchunks
    const remainingChunks = allChunks.slice(processedCount);
    const chunksToProcess = remainingChunks.slice(0, CHUNK_LIMIT);
    const newProcessedCount = processedCount + chunksToProcess.length;

    console.log(`üéØ This batch: processing ${chunksToProcess.length} chunks (${processedCount} ‚Üí ${newProcessedCount})`);

    // Step 4: Generate embeddings
    step = "generate embeddings";
    console.log(`
üß† Step 4: Generating embeddings...`);
    console.log(`
üìä Total texts to process: ${chunksToProcess.length}`);
    const embeddings = await generateEmbeddings(chunksToProcess);
    console.log(`‚úÖ Generated ${embeddings.length} embeddings`);

    // Step 5: Insert vectors into database (‰ΩøÁî®ÂÖ®Â±ÄchunkÁ¥¢Âºï)
    step = "insert vectors";
    console.log(`
üíæ Step 5: Inserting vectors into database...`);

    // ‰∏∫ÊØè‰∏™chunkÊ∑ªÂä†ÂÖ®Â±ÄÁ¥¢Âºï
    const chunksWithIndex = chunksToProcess.map((content, i) => ({
      content,
      index: processedCount + i  // ‰ΩøÁî®ÂÖ®Â±ÄÁ¥¢Âºï
    }));

    await db.insertVectorsWithIndex(fileId, chunksWithIndex, embeddings);
    console.log(`‚úÖ Inserted ${chunksToProcess.length} vectors`);

    // Step 6: Update upload record status
    step = "update status";
    console.log(`
üìä Step 6: Updating upload record status...`);

    // Âà§Êñ≠ÊòØÂê¶ÂÖ®ÈÉ®Â§ÑÁêÜÂÆåÊàê
    const isComplete = newProcessedCount >= allChunks.length;
    const newStatus = isComplete ? "completed" : "pending";

    await db.updateUploadRecord(fileId, newStatus, {
      chunks_count: allChunks.length,
      processed_chunks: newProcessedCount,
      ...(isComplete && { processed_at: new Date().toISOString() })
    });

    console.log(`‚úÖ Status updated to '${newStatus}' (${newProcessedCount}/${allChunks.length} chunks)`);

    console.log(`
${'='.repeat(60)}`);
    console.log(`‚ú® Batch processed: ${record.file_name}`);
    console.log(`${'='.repeat(60)}
`);

    return new Response(
      JSON.stringify({
        success: true,
        fileId,
        chunksProcessed: chunksToProcess.length,
        totalProcessed: newProcessedCount,
        totalChunks: allChunks.length,
        isComplete
      }),
      {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        status: 200
      }
    );

  } catch (error) {
    console.error(`
‚ùå Error at step '${step}':`, error);

    // Try to update the record status to 'failed'
    if (fileId) {
      try {
        const db = new SupabaseClient();
        await db.updateUploadRecord(fileId, "failed", {
          error_message: error.message,
          failed_at: new Date().toISOString(),
          failed_step: step
        });
        console.log(`üìù Updated record status to 'failed'`);
      } catch (updateError) {
        console.error(`‚ùå Failed to update error status:`, updateError);
      }
    }

    return new Response(
      JSON.stringify({
        error: error.message,
        step,
        fileId
      }),
      {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        status: 500
      }
    );
  }
});
