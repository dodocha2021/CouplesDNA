export class SupabaseClient {
  private url = Deno.env.get("SUPABASE_URL") ?? "";
  private key = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY") ?? "";

  async insertVectors(
    fileId: string,
    chunks: string[],
    embeddings: number[][]
  ): Promise<void> {
    const vectors = chunks.map((chunk, i) => ({
      content: chunk,
      embedding: embeddings[i],
      upload_id: fileId,
      metadata: {
        file_id: fileId,
        chunk_index: i
      }
    }));

    // åˆ†æ‰¹æ’å…¥ï¼Œæ¯æ‰¹ 400 æ¡
    const batchSize = 400;
    const totalBatches = Math.ceil(vectors.length / batchSize);

    console.log(`ğŸ“¦ Inserting ${vectors.length} vectors in ${totalBatches} batches (batch size: ${batchSize})`);

    for (let i = 0; i < vectors.length; i += batchSize) {
      const batch = vectors.slice(i, i + batchSize);
      const batchNum = Math.floor(i / batchSize) + 1;

      console.log(`ğŸ’¾ Inserting batch ${batchNum}/${totalBatches} (${batch.length} vectors)...`);

      const response = await fetch(`${this.url}/rest/v1/knowledge_vectors`, {
        method: "POST",
        headers: {
          "apikey": this.key,
          "Authorization": `Bearer ${this.key}`,
          "Content-Type": "application/json",
          "Prefer": "return=minimal"
        },
        body: JSON.stringify(batch)
      });

      if (!response.ok) {
        const error = await response.text();
        throw new Error(`Failed to insert vectors batch ${batchNum}/${totalBatches}: ${error}`);
      }

      console.log(`âœ… Batch ${batchNum}/${totalBatches} inserted (${i + batch.length}/${vectors.length} total)`);

      // æ‰¹æ¬¡ä¹‹é—´çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…æ•°æ®åº“å‹åŠ›è¿‡å¤§ï¼ˆæœ€åä¸€æ‰¹ä¸éœ€è¦å»¶è¿Ÿï¼‰
      if (i + batchSize < vectors.length) {
        await new Promise(resolve => setTimeout(resolve, 200));
      }
    }

    console.log(`âœ¨ All ${vectors.length} vectors inserted successfully`);
  }

  /**
   * å¸¦å…¨å±€ç´¢å¼•çš„å‘é‡æ’å…¥æ–¹æ³•ï¼Œæ”¯æŒsegmented processing
   * è‡ªé€‚åº”é‡è¯•ï¼š400 â†’ 200 â†’ 100 â†’ 50
   */
  async insertVectorsWithIndex(
    fileId: string,
    chunksWithIndex: Array<{ content: string; index: number }>,
    embeddings: number[][]
  ): Promise<void> {
    const vectors = chunksWithIndex.map((chunk, i) => ({
      content: chunk.content,
      embedding: embeddings[i],
      upload_id: fileId,
      metadata: {
        file_id: fileId,
        chunk_index: chunk.index  // ä½¿ç”¨å…¨å±€chunkç´¢å¼•
      }
    }));

    console.log(`ğŸ“¦ Inserting ${vectors.length} vectors with adaptive batch size`);

    await this.insertVectorsAdaptive(vectors);

    console.log(`âœ… Inserted ${vectors.length} vectors`);
  }

  /**
   * è‡ªé€‚åº”æ‰¹é‡æ’å…¥ï¼šæ”¯æŒbatch sizeé™çº§å’Œé‡è¯•
   */
  private async insertVectorsAdaptive(
    vectors: Array<{
      content: string;
      embedding: number[];
      upload_id: string;
      metadata: { file_id: string; chunk_index: number };
    }>
  ): Promise<void> {
    const batchSizes = [400, 200, 100, 50]; // é™çº§ç­–ç•¥
    let currentBatchSizeIndex = 0;
    let batchSize = batchSizes[currentBatchSizeIndex];

    const totalBatches = Math.ceil(vectors.length / batchSize);
    console.log(`ğŸ“¦ Inserting ${vectors.length} vectors in ${totalBatches} batches (initial batch size: ${batchSize})`);

    let i = 0;
    while (i < vectors.length) {
      const batch = vectors.slice(i, i + batchSize);
      const batchNum = Math.floor(i / batchSize) + 1;

      console.log(`ğŸ’¾ Inserting batch ${batchNum} (${batch.length} vectors, batch size: ${batchSize})...`);

      try {
        const response = await fetch(`${this.url}/rest/v1/knowledge_vectors`, {
          method: "POST",
          headers: {
            "apikey": this.key,
            "Authorization": `Bearer ${this.key}`,
            "Content-Type": "application/json",
            "Prefer": "return=minimal"
          },
          body: JSON.stringify(batch)
        });

        if (!response.ok) {
          const error = await response.text();
          throw new Error(`Failed to insert: ${error}`);
        }

        console.log(`âœ… Batch ${batchNum} inserted (${i + batch.length}/${vectors.length} total)`);

        // æˆåŠŸï¼šç»§ç»­ä¸‹ä¸€æ‰¹
        i += batchSize;

        // æ‰¹æ¬¡ä¹‹é—´å»¶è¿Ÿ200msï¼ˆæœ€åä¸€æ‰¹ä¸éœ€è¦å»¶è¿Ÿï¼‰
        if (i < vectors.length) {
          await new Promise(resolve => setTimeout(resolve, 200));
        }

      } catch (error) {
        const errorMsg = error instanceof Error ? error.message : String(error);
        console.error(`âŒ Batch ${batchNum} failed: ${errorMsg}`);

        // å¦‚æœæ˜¯è¶…æ—¶é”™è¯¯ï¼Œå°è¯•é™çº§batch size
        if (errorMsg.includes('57014') || errorMsg.includes('timeout')) {
          if (currentBatchSizeIndex < batchSizes.length - 1) {
            // é™çº§åˆ°æ›´å°çš„batch size
            currentBatchSizeIndex++;
            batchSize = batchSizes[currentBatchSizeIndex];

            console.log(`ğŸ”„ Reducing batch size to ${batchSize}, waiting 1 second before retry...`);
            await new Promise(resolve => setTimeout(resolve, 1000));

            // ä¸å¢åŠ  iï¼Œé‡è¯•å½“å‰batchï¼ˆç”¨æ–°çš„batch sizeï¼‰
            continue;
          } else {
            // å·²ç»æ˜¯æœ€å°batch sizeè¿˜å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
            throw new Error(`Failed to insert vectors even with smallest batch size (${batchSize}): ${errorMsg}`);
          }
        } else {
          // éè¶…æ—¶é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
          throw error;
        }
      }
    }

    console.log(`âœ¨ All ${vectors.length} vectors inserted successfully`);
  }

  /**
   * æ›´æ–°ä¸Šä¼ è®°å½•çŠ¶æ€ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
   */
  async updateUploadRecord(
    fileId: string,
    status: string,
    metadataUpdate?: Record<string, any>
  ): Promise<void> {
    const maxRetries = 5;
    let lastError: Error | null = null;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        // ç¬¬1æ­¥ï¼šè·å–ç°æœ‰metadata
        const getResponse = await fetch(
          `${this.url}/rest/v1/knowledge_uploads?id=eq.${fileId}&select=metadata`,
          {
            method: "GET",
            headers: { "apikey": this.key, "Authorization": `Bearer ${this.key}` },
          }
        );

        if (!getResponse.ok) {
          throw new Error("Failed to fetch existing metadata");
        }

        const existingData = await getResponse.json();
        const existingMetadata = existingData[0]?.metadata || {};

        // ç¬¬2æ­¥ï¼šåˆå¹¶å¹¶æ›´æ–°
        const updateBody = {
          status,
          metadata: { ...existingMetadata, ...metadataUpdate },
        };

        const response = await fetch(
          `${this.url}/rest/v1/knowledge_uploads?id=eq.${fileId}`,
          {
            method: "PATCH",
            headers: {
              "apikey": this.key,
              "Authorization": `Bearer ${this.key}`,
              "Content-Type": "application/json",
            },
            body: JSON.stringify(updateBody),
          }
        );

        if (!response.ok) {
          const error = await response.text();
          throw new Error(`Failed to update record: ${error}`);
        }

        // æˆåŠŸï¼Œè¿”å›
        if (attempt > 1) {
          console.log(`âœ… Update succeeded on attempt ${attempt}`);
        }
        return;

      } catch (error) {
        lastError = error instanceof Error ? error : new Error(String(error));
        const errorMsg = lastError.message;

        console.error(`âŒ Update attempt ${attempt}/${maxRetries} failed: ${errorMsg}`);

        // å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç­‰å¾…åé‡è¯•
        if (attempt < maxRetries) {
          const waitTime = attempt * 1000; // 1s, 2s, 3s, 4s, 5s
          console.log(`ğŸ”„ Waiting ${waitTime}ms before retry...`);
          await new Promise(resolve => setTimeout(resolve, waitTime));
        }
      }
    }

    // æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    throw new Error(`Failed to update record after ${maxRetries} attempts: ${lastError?.message}`);
  }
}
